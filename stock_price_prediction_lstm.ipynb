{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH-yZlXtt6sP"
      },
      "source": [
        "**STOCK PRICE PREDICTION USING LSTM**\n",
        "\n",
        "Let's start by installing required packages and import them and the relevant data for stock prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y_ooFNUreUZ",
        "outputId": "ef6c4372-ccd5-416c-93e7-119289dc476a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.10/dist-packages (0.2.33)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.23.5)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.31.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.9.4)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2023.3.post1)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.3.10)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.10/dist-packages (from yfinance) (3.17.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.11.2)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2023.11.17)\n",
            "ERROR: unknown command \"instal\" - maybe you meant \"install\"\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Collecting datetime\n",
            "  Downloading DateTime-5.4-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zope.interface (from datetime)\n",
            "  Downloading zope.interface-6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.1/247.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from datetime) (2023.3.post1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface->datetime) (67.7.2)\n",
            "Installing collected packages: zope.interface, datetime\n",
            "Successfully installed datetime-5.4 zope.interface-6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance\n",
        "!pip instal numpy\n",
        "!pip install tensorflow\n",
        "!pip install matplotlib\n",
        "!pip install datetime\n",
        "\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import LSTM,Dense,SimpleRNN,Dropout,Add,Concatenate,Flatten\n",
        "from tensorflow.keras import Model,Input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq5tSu6vhq4H"
      },
      "source": [
        "You can fill the name of the option as you wish in the next line of code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UZ9g64nssj7r"
      },
      "outputs": [],
      "source": [
        "info_stock = yf.Ticker(\"SASA.IS\")\n",
        "data_stock = info_stock.history(period=\"max\")\n",
        "data_stock = data_stock.sort_values('Date',ascending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YHnnd5SDtz2a"
      },
      "outputs": [],
      "source": [
        "print(type(data_stock))\n",
        "data_stock.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IE6avuIuYQM"
      },
      "source": [
        "Stock price data is apparent since 2000 on a daily basis. Obviously, average prices of stocks are not provided but open and end prices and lowest and highest prices during the day are available. In addition, days when the dividend payments are made are provided as well as stock splits showing the changes in the issued number of stocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ld6ZP5GivhDw"
      },
      "outputs": [],
      "source": [
        "print(f\"Total Number of Dividend Payments: {sum(data_stock['Dividends'].values > 0)}\")\n",
        "print(f\"Total Number of Stock Splits: {sum(data_stock['Stock Splits'].values > 0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-FRI9JVwydZ"
      },
      "source": [
        "The changes in the stock prices in Dividend Days or Stock Split Days might be expected to be drastic. Let's see that"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bvjF0O2_w_0V"
      },
      "outputs": [],
      "source": [
        "dividend_days = [index for index,value in enumerate(data_stock['Dividends'].values) if value > 0]\n",
        "dividend_selected_days = sorted(np.concatenate([np.array(dividend_days) - 1,np.array(dividend_days),np.array(dividend_days) + 1],axis=0))\n",
        "data_stock.iloc[np.array(dividend_selected_days),:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhHMfVIW0tVu"
      },
      "source": [
        "On these two days, it seems that dividend payments were made. There didn't seem to occur any drops/jumps in the prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRZOwfu31ZAS"
      },
      "outputs": [],
      "source": [
        "split_days = [index for index,value in enumerate(data_stock['Stock Splits'].values) if value > 0]\n",
        "split_selected_days = sorted(np.concatenate([np.array(split_days[-2:]) - 1,np.array(split_days[-2:]),np.array(split_days[-2:]) + 1],axis=0))\n",
        "data_stock.iloc[np.array(split_selected_days),:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMvIuoeD16Qz"
      },
      "source": [
        "On these 2 days, it seems like there occured a stock split which lead to a drastic stock price decrease on 23rd but didn't seem to change the prices on the other split.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUnyiPt0Dl_f"
      },
      "outputs": [],
      "source": [
        "def plot_function(seriesx,seriesy,labelx,labely,title):\n",
        "\n",
        "  for i in range(len(seriesx)):\n",
        "    if len(seriesx) != 1:\n",
        "      plt.subplot(1,2,i+1)\n",
        "    plt.plot(seriesx[i],seriesy[i])\n",
        "    plt.xlabel(labelx)\n",
        "    plt.ylabel(labely)\n",
        "    plt.title(title)\n",
        "    plt.xticks(rotation=50)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plot_function([data_stock.index,data_stock.index[-500:]],[data_stock.Close,data_stock.Close[-500:]],\"Date\",\"Stock Price\",\"Date vs. Stock Price\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEw2_S1G_chV"
      },
      "source": [
        "Let's convert the model to a numpy array and then to tensorflow dataset to transorm the data into window format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_VfV95sUOlo"
      },
      "source": [
        "Let's set the batch size to a small number as 2 which is pretty much inline with SGD. In the last experiment, the batch size was quite high making the iterations more stable and faster but the model seemed to get stuck at a local minimum so lowering the batch size might help minimizing loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9t7SXygca2o"
      },
      "source": [
        "The values should be standardized as min max scaling the values might diminish training pace."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of passing open, close, high etc. as they are, the current value will be passed as the ratio of current value to the previous value i.e., $\\frac{y^t_{open}}{y^{t-1}_{open}}$ will be passed in the estimation of $y^t_{close}$ instead of passing $y^t_{open}$ solely."
      ],
      "metadata": {
        "id": "KDHyPueTXuvu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifQK4VVBA90J"
      },
      "outputs": [],
      "source": [
        "window_size = 20\n",
        "batch_size = 2\n",
        "lastndays = 200\n",
        "epochs = 100\n",
        "val_start_date = pd.Timestamp('2023-01-01', tz='Europe/Istanbul')\n",
        "norm_mean_vector = tf.convert_to_tensor(np.mean(data_stock,axis=0).astype('float32'))\n",
        "norm_std_vector = tf.convert_to_tensor(np.std(data_stock,axis=0).astype('float32'))\n",
        "close_mean = norm_mean_vector[3]\n",
        "close_std = norm_std_vector[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCVWHDoI23E6",
        "outputId": "4ba392b0-6bde-451b-837c-23d390a7a568"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of first batch of data: (2, 20, 8)\n",
            "Shape of first batch of response data: (2,)\n"
          ]
        }
      ],
      "source": [
        "val_start_date_ix = [x for x,y in enumerate(list(data_stock.index)) if y >= val_start_date][0]\n",
        "open_series = data_stock['Open'].values\n",
        "close_series = data_stock['Close'].values\n",
        "max_close_val = max(close_series)\n",
        "\n",
        "def convert_data(data_matrix):\n",
        "\n",
        "  coln = data_matrix.shape[-1]\n",
        "  rown = data_matrix.shape[-2]\n",
        "  close_val = data_matrix[-1,3]\n",
        "  open_val = data_matrix[-1,0]\n",
        "  data_matrix = tf.concat([data_matrix[1:window_size+1,:4] / data_matrix[:window_size,:4],data_matrix[1:window_size+1,4:]],axis=-1)\n",
        "  tfz = tf.zeros(shape=[(coln-1)],dtype='float32')\n",
        "  tfz = tf.concat([data_matrix[-1,0][tf.newaxis],tfz],0)\n",
        "  tfz2 = tf.zeros(shape=[(window_size-1)],dtype='float32')\n",
        "  tfz2 = tf.concat([open_val[tf.newaxis],tfz2],0)\n",
        "  data_matrix = (tf.concat([data_matrix[:-1,:],tfz[tf.newaxis]],0)-norm_mean_vector[None,:])/norm_std_vector[None,:]\n",
        "  data_matrix = tf.concat([data_matrix,tfz2[:,tf.newaxis]],axis=-1)\n",
        "\n",
        "  return (data_matrix,close_val)\n",
        "\n",
        "def preprocess_data(sequence_data, data_type):\n",
        "\n",
        "  row_sequence = sequence_data.shape[0]\n",
        "  column_sequence = sequence_data.shape[1]\n",
        "  sequence_data = np.array(sequence_data,dtype='float32')\n",
        "  sequence_data = tf.data.Dataset.from_tensor_slices(sequence_data)\n",
        "  sequence_data = sequence_data.window(size = window_size + 1, shift = 1, drop_remainder = True)\n",
        "  sequence_data = sequence_data.flat_map(lambda x: x.batch(window_size+1))\n",
        "  sequence_data = sequence_data.map(lambda x: convert_data(x))\n",
        "  if data_type == 'training':\n",
        "    sequence_data = sequence_data.shuffle(row_sequence)\n",
        "  sequence_data = sequence_data.batch(batch_size).prefetch(1)\n",
        "  return sequence_data\n",
        "\n",
        "training_data = preprocess_data(data_stock[:val_start_date_ix],'training')\n",
        "validation_data = preprocess_data(data_stock[(val_start_date_ix-window_size):],'validation')\n",
        "\n",
        "for i in validation_data:\n",
        "  print(f\"Shape of first batch of data: {i[0].shape}\")\n",
        "  print(f\"Shape of first batch of response data: {i[1].shape}\")\n",
        "  break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPAn4ipIDGSs"
      },
      "source": [
        "As shown above, each batch is composed of 2 observations and 11 days of window with all the previous information to be used. The close value of the day will be estimated using previous 10 days of information in addition to current day's open price as the rest of the information such as close, high, low are masked."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKnQpXmtWV-I"
      },
      "source": [
        "Let's see if using previous close data point in the estimation of next gives reasonable values or not and also let's use current day's open value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVcjpIytTlzW",
        "outputId": "32a63f67-b291-4287-8397-6de420f36a99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Absolute Percentage All Data Error for Adhoc Method using Close: 1.9531615272103435\n",
            "Mean Absolute Percentage Validation Error for Adhoc Method using Close: 3.3452645056895025\n",
            "Mean Absolute Percentage All Data Error for Adhoc Method using Open: 1.0780315708514465\n",
            "Mean Absolute Percentage Validation Error for Adhoc Method using Open: 2.399660708500607\n"
          ]
        }
      ],
      "source": [
        "mape_adhoc = tf.keras.metrics.mean_absolute_percentage_error(close_series[1:val_start_date_ix],close_series[:val_start_date_ix-1]).numpy()\n",
        "print(f\"Mean Absolute Percentage All Data Error for Adhoc Method using Close: {mape_adhoc}\")\n",
        "mape_adhoc = tf.keras.metrics.mean_absolute_percentage_error(close_series[val_start_date_ix:],close_series[val_start_date_ix-1:-1]).numpy()\n",
        "print(f\"Mean Absolute Percentage Validation Error for Adhoc Method using Close: {mape_adhoc}\")\n",
        "mape_adhoc = tf.keras.metrics.mean_absolute_percentage_error(close_series[1:val_start_date_ix],open_series[1:val_start_date_ix]).numpy()\n",
        "print(f\"Mean Absolute Percentage All Data Error for Adhoc Method using Open: {mape_adhoc}\")\n",
        "mape_adhoc = tf.keras.metrics.mean_absolute_percentage_error(close_series[val_start_date_ix:],open_series[val_start_date_ix:]).numpy()\n",
        "print(f\"Mean Absolute Percentage Validation Error for Adhoc Method using Open: {mape_adhoc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pITmip81LPXo"
      },
      "source": [
        "In both training and the validation set, the close values are less accurate in the estimation of next day's close values compared to current day's open values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v22C0zieYrT8"
      },
      "source": [
        "Optimizing MAPE should make more sense than any other prevalent metrics such as MSE or MAE as it mimics the return which could be obtained out of investing the derivative. We will adopt a more conservative model with a shallow depth to make the training easier as the gradient will flow through more freely when the network is shallower.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EamfNL3WcE3K"
      },
      "source": [
        "$$\n",
        "y_{close}^t = y_{open}^{t}\\big(1+ r(x^{t-1},x^{t-2},\\ldots,x^{t-10})\\big)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkJLbToZcuB4"
      },
      "source": [
        "The formula above shows how the model architecture is designed the $close$ value of the price at time $t$ will be estimated using $open$ price at time $t$ multiplied by $1 + \\text{Estimated change in the value of price since the open price}$. $r$ will be estimated using a $tanh$ activation at the top layer assuming the price might go all the way down to 0 and might be doubled where both boundaries are assumed to be extreme."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tls-qArJfsgh"
      },
      "source": [
        "`ReduceLROnPlateau` will be used in optimizing the loss function as it reduces the learning rate by multiplying the weight by a factor of $f$ which is executed at every $p$ iterations where the loss cannot be minimized further"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQ7iXQ5vY5GD",
        "outputId": "5865ab1e-caae-47cb-bc8d-d38376afb893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "2914/2914 [==============================] - 44s 13ms/step - loss: 1.1163 - mse: 0.1106 - mae: 0.0460 - val_loss: 2.4050 - val_mse: 2.3482 - val_mae: 1.1821 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "2914/2914 [==============================] - 38s 13ms/step - loss: 1.1104 - mse: 0.1108 - mae: 0.0460 - val_loss: 2.4022 - val_mse: 2.3465 - val_mae: 1.1808 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.1102 - mse: 0.1108 - mae: 0.0461 - val_loss: 2.4173 - val_mse: 2.3560 - val_mae: 1.1878 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.1082 - mse: 0.1105 - mae: 0.0459 - val_loss: 2.4066 - val_mse: 2.3491 - val_mae: 1.1828 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "2914/2914 [==============================] - 38s 13ms/step - loss: 1.1072 - mse: 0.1099 - mae: 0.0460 - val_loss: 2.4200 - val_mse: 2.3579 - val_mae: 1.1891 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "2914/2914 [==============================] - 38s 13ms/step - loss: 1.1077 - mse: 0.1108 - mae: 0.0461 - val_loss: 2.3963 - val_mse: 2.3425 - val_mae: 1.1780 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.1038 - mse: 0.1111 - mae: 0.0460 - val_loss: 2.3942 - val_mse: 2.3410 - val_mae: 1.1771 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "2914/2914 [==============================] - 38s 13ms/step - loss: 1.1069 - mse: 0.1109 - mae: 0.0460 - val_loss: 2.3925 - val_mse: 2.3399 - val_mae: 1.1763 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.1069 - mse: 0.1111 - mae: 0.0460 - val_loss: 2.4262 - val_mse: 2.3622 - val_mae: 1.1920 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.1057 - mse: 0.1107 - mae: 0.0459 - val_loss: 2.3883 - val_mse: 2.3370 - val_mae: 1.1744 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "2914/2914 [==============================] - 38s 13ms/step - loss: 1.1023 - mse: 0.1102 - mae: 0.0459 - val_loss: 2.4113 - val_mse: 2.3521 - val_mae: 1.1850 - lr: 8.0000e-04\n",
            "Epoch 12/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.1022 - mse: 0.1113 - mae: 0.0460 - val_loss: 2.3726 - val_mse: 2.3273 - val_mae: 1.1673 - lr: 8.0000e-04\n",
            "Epoch 13/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.1019 - mse: 0.1108 - mae: 0.0460 - val_loss: 2.3975 - val_mse: 2.3434 - val_mae: 1.1786 - lr: 8.0000e-04\n",
            "Epoch 14/200\n",
            "2914/2914 [==============================] - 38s 13ms/step - loss: 1.1019 - mse: 0.1110 - mae: 0.0460 - val_loss: 2.3575 - val_mse: 2.3212 - val_mae: 1.1605 - lr: 8.0000e-04\n",
            "Epoch 15/200\n",
            "2914/2914 [==============================] - 38s 13ms/step - loss: 1.0980 - mse: 0.1111 - mae: 0.0460 - val_loss: 2.3885 - val_mse: 2.3372 - val_mae: 1.1745 - lr: 8.0000e-04\n",
            "Epoch 16/200\n",
            "2914/2914 [==============================] - 38s 13ms/step - loss: 1.1003 - mse: 0.1105 - mae: 0.0459 - val_loss: 2.4273 - val_mse: 2.3630 - val_mae: 1.1925 - lr: 8.0000e-04\n",
            "Epoch 17/200\n",
            "2914/2914 [==============================] - 38s 13ms/step - loss: 1.1023 - mse: 0.1110 - mae: 0.0460 - val_loss: 2.3894 - val_mse: 2.3378 - val_mae: 1.1749 - lr: 8.0000e-04\n",
            "Epoch 18/200\n",
            "2914/2914 [==============================] - 38s 13ms/step - loss: 1.1041 - mse: 0.1109 - mae: 0.0460 - val_loss: 2.3954 - val_mse: 2.3419 - val_mae: 1.1776 - lr: 8.0000e-04\n",
            "Epoch 19/200\n",
            "2914/2914 [==============================] - 38s 13ms/step - loss: 1.1017 - mse: 0.1108 - mae: 0.0460 - val_loss: 2.4094 - val_mse: 2.3508 - val_mae: 1.1841 - lr: 6.4000e-04\n",
            "Epoch 20/200\n",
            "2914/2914 [==============================] - 38s 13ms/step - loss: 1.0962 - mse: 0.1108 - mae: 0.0460 - val_loss: 2.3933 - val_mse: 2.3404 - val_mae: 1.1767 - lr: 6.4000e-04\n",
            "Epoch 21/200\n",
            "2914/2914 [==============================] - 38s 13ms/step - loss: 1.0987 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3952 - val_mse: 2.3418 - val_mae: 1.1776 - lr: 6.4000e-04\n",
            "Epoch 22/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.0990 - mse: 0.1108 - mae: 0.0460 - val_loss: 2.4042 - val_mse: 2.3477 - val_mae: 1.1817 - lr: 6.4000e-04\n",
            "Epoch 23/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.0984 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3917 - val_mse: 2.3393 - val_mae: 1.1759 - lr: 6.4000e-04\n",
            "Epoch 24/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.0948 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.4089 - val_mse: 2.3505 - val_mae: 1.1839 - lr: 5.1200e-04\n",
            "Epoch 25/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.0938 - mse: 0.1109 - mae: 0.0460 - val_loss: 2.4033 - val_mse: 2.3471 - val_mae: 1.1813 - lr: 5.1200e-04\n",
            "Epoch 26/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.0963 - mse: 0.1105 - mae: 0.0459 - val_loss: 2.3913 - val_mse: 2.3391 - val_mae: 1.1758 - lr: 5.1200e-04\n",
            "Epoch 27/200\n",
            "2914/2914 [==============================] - 38s 13ms/step - loss: 1.0941 - mse: 0.1110 - mae: 0.0460 - val_loss: 2.4002 - val_mse: 2.3453 - val_mae: 1.1798 - lr: 5.1200e-04\n",
            "Epoch 28/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.0935 - mse: 0.1108 - mae: 0.0460 - val_loss: 2.4074 - val_mse: 2.3496 - val_mae: 1.1832 - lr: 5.1200e-04\n",
            "Epoch 29/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.0937 - mse: 0.1108 - mae: 0.0460 - val_loss: 2.3980 - val_mse: 2.3438 - val_mae: 1.1788 - lr: 5.1200e-04\n",
            "Epoch 30/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.0952 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3977 - val_mse: 2.3435 - val_mae: 1.1787 - lr: 5.1200e-04\n",
            "Epoch 31/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.0946 - mse: 0.1110 - mae: 0.0460 - val_loss: 2.4121 - val_mse: 2.3526 - val_mae: 1.1854 - lr: 5.1200e-04\n",
            "Epoch 32/200\n",
            "2914/2914 [==============================] - 38s 13ms/step - loss: 1.0909 - mse: 0.1107 - mae: 0.0459 - val_loss: 2.4029 - val_mse: 2.3469 - val_mae: 1.1811 - lr: 4.0960e-04\n",
            "Epoch 33/200\n",
            "2914/2914 [==============================] - 38s 13ms/step - loss: 1.0904 - mse: 0.1109 - mae: 0.0459 - val_loss: 2.3886 - val_mse: 2.3372 - val_mae: 1.1745 - lr: 4.0960e-04\n",
            "Epoch 34/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.0915 - mse: 0.1109 - mae: 0.0460 - val_loss: 2.3999 - val_mse: 2.3451 - val_mae: 1.1797 - lr: 4.0960e-04\n",
            "Epoch 35/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.0922 - mse: 0.1108 - mae: 0.0460 - val_loss: 2.4004 - val_mse: 2.3454 - val_mae: 1.1799 - lr: 4.0960e-04\n",
            "Epoch 36/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0908 - mse: 0.1108 - mae: 0.0460 - val_loss: 2.3937 - val_mse: 2.3407 - val_mae: 1.1769 - lr: 4.0960e-04\n",
            "Epoch 37/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0896 - mse: 0.1108 - mae: 0.0460 - val_loss: 2.3978 - val_mse: 2.3436 - val_mae: 1.1787 - lr: 3.2768e-04\n",
            "Epoch 38/200\n",
            "2914/2914 [==============================] - 38s 13ms/step - loss: 1.0897 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3996 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 3.2768e-04\n",
            "Epoch 39/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0907 - mse: 0.1108 - mae: 0.0459 - val_loss: 2.3967 - val_mse: 2.3428 - val_mae: 1.1782 - lr: 3.2768e-04\n",
            "Epoch 40/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0892 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3995 - val_mse: 2.3449 - val_mae: 1.1795 - lr: 3.2768e-04\n",
            "Epoch 41/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0901 - mse: 0.1108 - mae: 0.0460 - val_loss: 2.3966 - val_mse: 2.3427 - val_mae: 1.1782 - lr: 3.2768e-04\n",
            "Epoch 42/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0894 - mse: 0.1108 - mae: 0.0460 - val_loss: 2.4037 - val_mse: 2.3474 - val_mae: 1.1815 - lr: 3.2768e-04\n",
            "Epoch 43/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0897 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3904 - val_mse: 2.3384 - val_mae: 1.1753 - lr: 3.2768e-04\n",
            "Epoch 44/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0881 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3959 - val_mse: 2.3423 - val_mae: 1.1779 - lr: 2.6214e-04\n",
            "Epoch 45/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0873 - mse: 0.1106 - mae: 0.0460 - val_loss: 2.3995 - val_mse: 2.3449 - val_mae: 1.1795 - lr: 2.6214e-04\n",
            "Epoch 46/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0885 - mse: 0.1106 - mae: 0.0460 - val_loss: 2.3988 - val_mse: 2.3444 - val_mae: 1.1792 - lr: 2.6214e-04\n",
            "Epoch 47/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0875 - mse: 0.1106 - mae: 0.0460 - val_loss: 2.4027 - val_mse: 2.3468 - val_mae: 1.1810 - lr: 2.6214e-04\n",
            "Epoch 48/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0882 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 2.6214e-04\n",
            "Epoch 49/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0866 - mse: 0.1106 - mae: 0.0460 - val_loss: 2.3988 - val_mse: 2.3444 - val_mae: 1.1792 - lr: 2.0972e-04\n",
            "Epoch 50/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0872 - mse: 0.1107 - mae: 0.0459 - val_loss: 2.4121 - val_mse: 2.3525 - val_mae: 1.1854 - lr: 2.0972e-04\n",
            "Epoch 51/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0873 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3984 - val_mse: 2.3440 - val_mae: 1.1790 - lr: 2.0972e-04\n",
            "Epoch 52/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0870 - mse: 0.1109 - mae: 0.0460 - val_loss: 2.4046 - val_mse: 2.3479 - val_mae: 1.1819 - lr: 2.0972e-04\n",
            "Epoch 53/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0850 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.4007 - val_mse: 2.3456 - val_mae: 1.1801 - lr: 1.6777e-04\n",
            "Epoch 54/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0854 - mse: 0.1108 - mae: 0.0460 - val_loss: 2.3981 - val_mse: 2.3438 - val_mae: 1.1789 - lr: 1.6777e-04\n",
            "Epoch 55/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0865 - mse: 0.1106 - mae: 0.0460 - val_loss: 2.3996 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.6777e-04\n",
            "Epoch 56/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0860 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3894 - val_mse: 2.3378 - val_mae: 1.1749 - lr: 1.6777e-04\n",
            "Epoch 57/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0855 - mse: 0.1107 - mae: 0.0459 - val_loss: 2.3946 - val_mse: 2.3413 - val_mae: 1.1773 - lr: 1.3422e-04\n",
            "Epoch 58/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0848 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3993 - val_mse: 2.3447 - val_mae: 1.1794 - lr: 1.3422e-04\n",
            "Epoch 59/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0852 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3977 - val_mse: 2.3435 - val_mae: 1.1787 - lr: 1.3422e-04\n",
            "Epoch 60/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0850 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3991 - val_mse: 2.3446 - val_mae: 1.1794 - lr: 1.3422e-04\n",
            "Epoch 61/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.0850 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3959 - val_mse: 2.3422 - val_mae: 1.1779 - lr: 1.3422e-04\n",
            "Epoch 62/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0842 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3983 - val_mse: 2.3440 - val_mae: 1.1790 - lr: 1.0737e-04\n",
            "Epoch 63/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0843 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3990 - val_mse: 2.3445 - val_mae: 1.1793 - lr: 1.0737e-04\n",
            "Epoch 64/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0839 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3974 - val_mse: 2.3433 - val_mae: 1.1786 - lr: 1.0737e-04\n",
            "Epoch 65/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0846 - mse: 0.1108 - mae: 0.0460 - val_loss: 2.3993 - val_mse: 2.3447 - val_mae: 1.1794 - lr: 1.0737e-04\n",
            "Epoch 66/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0842 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.4018 - val_mse: 2.3463 - val_mae: 1.1806 - lr: 1.0737e-04\n",
            "Epoch 67/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0844 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.4004 - val_mse: 2.3454 - val_mae: 1.1799 - lr: 1.0737e-04\n",
            "Epoch 68/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0835 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3992 - val_mse: 2.3447 - val_mae: 1.1794 - lr: 8.5899e-05\n",
            "Epoch 69/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0837 - mse: 0.1107 - mae: 0.0459 - val_loss: 2.4003 - val_mse: 2.3454 - val_mae: 1.1799 - lr: 8.5899e-05\n",
            "Epoch 70/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0837 - mse: 0.1106 - mae: 0.0459 - val_loss: 2.4011 - val_mse: 2.3458 - val_mae: 1.1803 - lr: 8.5899e-05\n",
            "Epoch 71/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0837 - mse: 0.1108 - mae: 0.0460 - val_loss: 2.3995 - val_mse: 2.3449 - val_mae: 1.1795 - lr: 8.5899e-05\n",
            "Epoch 72/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0832 - mse: 0.1107 - mae: 0.0459 - val_loss: 2.4002 - val_mse: 2.3453 - val_mae: 1.1798 - lr: 6.8719e-05\n",
            "Epoch 73/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0831 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3968 - val_mse: 2.3429 - val_mae: 1.1783 - lr: 6.8719e-05\n",
            "Epoch 74/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0834 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3999 - val_mse: 2.3451 - val_mae: 1.1797 - lr: 6.8719e-05\n",
            "Epoch 75/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0831 - mse: 0.1106 - mae: 0.0459 - val_loss: 2.3988 - val_mse: 2.3443 - val_mae: 1.1792 - lr: 6.8719e-05\n",
            "Epoch 76/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0832 - mse: 0.1106 - mae: 0.0459 - val_loss: 2.3995 - val_mse: 2.3449 - val_mae: 1.1795 - lr: 6.8719e-05\n",
            "Epoch 77/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0830 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3992 - val_mse: 2.3447 - val_mae: 1.1794 - lr: 5.4976e-05\n",
            "Epoch 78/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0829 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3993 - val_mse: 2.3447 - val_mae: 1.1794 - lr: 5.4976e-05\n",
            "Epoch 79/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0830 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.4003 - val_mse: 2.3454 - val_mae: 1.1799 - lr: 5.4976e-05\n",
            "Epoch 80/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0829 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3987 - val_mse: 2.3443 - val_mae: 1.1791 - lr: 5.4976e-05\n",
            "Epoch 81/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0826 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3994 - val_mse: 2.3448 - val_mae: 1.1795 - lr: 4.3980e-05\n",
            "Epoch 82/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0826 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3995 - val_mse: 2.3449 - val_mae: 1.1795 - lr: 4.3980e-05\n",
            "Epoch 83/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0825 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3991 - val_mse: 2.3446 - val_mae: 1.1793 - lr: 4.3980e-05\n",
            "Epoch 84/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0828 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.4005 - val_mse: 2.3455 - val_mae: 1.1800 - lr: 4.3980e-05\n",
            "Epoch 85/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0825 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3999 - val_mse: 2.3452 - val_mae: 1.1797 - lr: 3.5184e-05\n",
            "Epoch 86/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0824 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3999 - val_mse: 2.3452 - val_mae: 1.1797 - lr: 3.5184e-05\n",
            "Epoch 87/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0824 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.4000 - val_mse: 2.3452 - val_mae: 1.1798 - lr: 3.5184e-05\n",
            "Epoch 88/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0824 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3995 - val_mse: 2.3449 - val_mae: 1.1795 - lr: 3.5184e-05\n",
            "Epoch 89/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0823 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3998 - val_mse: 2.3451 - val_mae: 1.1797 - lr: 2.8147e-05\n",
            "Epoch 90/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0823 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3995 - val_mse: 2.3449 - val_mae: 1.1795 - lr: 2.8147e-05\n",
            "Epoch 91/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0822 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3996 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 2.8147e-05\n",
            "Epoch 92/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0823 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.4005 - val_mse: 2.3455 - val_mae: 1.1800 - lr: 2.8147e-05\n",
            "Epoch 93/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0822 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.4000 - val_mse: 2.3452 - val_mae: 1.1798 - lr: 2.2518e-05\n",
            "Epoch 94/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0821 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3992 - val_mse: 2.3447 - val_mae: 1.1794 - lr: 2.2518e-05\n",
            "Epoch 95/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0821 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.4003 - val_mse: 2.3454 - val_mae: 1.1799 - lr: 2.2518e-05\n",
            "Epoch 96/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0821 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3451 - val_mae: 1.1796 - lr: 2.2518e-05\n",
            "Epoch 97/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0821 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3994 - val_mse: 2.3448 - val_mae: 1.1795 - lr: 2.2518e-05\n",
            "Epoch 98/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0820 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3995 - val_mse: 2.3449 - val_mae: 1.1795 - lr: 1.8014e-05\n",
            "Epoch 99/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0820 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3996 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.8014e-05\n",
            "Epoch 100/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.0820 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3992 - val_mse: 2.3447 - val_mae: 1.1794 - lr: 1.8014e-05\n",
            "Epoch 101/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0820 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3992 - val_mse: 2.3447 - val_mae: 1.1794 - lr: 1.8014e-05\n",
            "Epoch 102/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0820 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3451 - val_mae: 1.1796 - lr: 1.8014e-05\n",
            "Epoch 103/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0819 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.4412e-05\n",
            "Epoch 104/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0819 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.4412e-05\n",
            "Epoch 105/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0819 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3995 - val_mse: 2.3449 - val_mae: 1.1795 - lr: 1.4412e-05\n",
            "Epoch 106/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0818 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.1529e-05\n",
            "Epoch 107/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0818 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3993 - val_mse: 2.3447 - val_mae: 1.1794 - lr: 1.1529e-05\n",
            "Epoch 108/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0819 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3995 - val_mse: 2.3449 - val_mae: 1.1795 - lr: 1.1529e-05\n",
            "Epoch 109/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.0819 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.1529e-05\n",
            "Epoch 110/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0818 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3996 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 9.2234e-06\n",
            "Epoch 111/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0818 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3451 - val_mae: 1.1796 - lr: 9.2234e-06\n",
            "Epoch 112/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0818 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3996 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 9.2234e-06\n",
            "Epoch 113/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0817 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3996 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 7.3787e-06\n",
            "Epoch 114/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0818 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3996 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 7.3787e-06\n",
            "Epoch 115/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.0817 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3996 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 7.3787e-06\n",
            "Epoch 116/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0817 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3998 - val_mse: 2.3451 - val_mae: 1.1796 - lr: 5.9030e-06\n",
            "Epoch 117/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0817 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3451 - val_mae: 1.1796 - lr: 5.9030e-06\n",
            "Epoch 118/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0817 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3996 - val_mse: 2.3449 - val_mae: 1.1795 - lr: 5.9030e-06\n",
            "Epoch 119/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0817 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 5.9030e-06\n",
            "Epoch 120/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0817 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3996 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 4.7224e-06\n",
            "Epoch 121/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3996 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 4.7224e-06\n",
            "Epoch 122/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0817 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3996 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 4.7224e-06\n",
            "Epoch 123/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 3.7779e-06\n",
            "Epoch 124/200\n",
            "2914/2914 [==============================] - 42s 14ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 3.7779e-06\n",
            "Epoch 125/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3451 - val_mae: 1.1796 - lr: 3.7779e-06\n",
            "Epoch 126/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3996 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 3.0223e-06\n",
            "Epoch 127/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3996 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 3.0223e-06\n",
            "Epoch 128/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 3.0223e-06\n",
            "Epoch 129/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 2.4179e-06\n",
            "Epoch 130/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3996 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 2.4179e-06\n",
            "Epoch 131/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3996 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 2.4179e-06\n",
            "Epoch 132/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3451 - val_mae: 1.1796 - lr: 2.4179e-06\n",
            "Epoch 133/200\n",
            "2914/2914 [==============================] - 42s 14ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.9343e-06\n",
            "Epoch 134/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3998 - val_mse: 2.3451 - val_mae: 1.1796 - lr: 1.9343e-06\n",
            "Epoch 135/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.9343e-06\n",
            "Epoch 136/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.5474e-06\n",
            "Epoch 137/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.5474e-06\n",
            "Epoch 138/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.5474e-06\n",
            "Epoch 139/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3996 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.2379e-06\n",
            "Epoch 140/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3996 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.2379e-06\n",
            "Epoch 141/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3996 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.2379e-06\n",
            "Epoch 142/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 9.9035e-07\n",
            "Epoch 143/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3996 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 9.9035e-07\n",
            "Epoch 144/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 9.9035e-07\n",
            "Epoch 145/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 7.9228e-07\n",
            "Epoch 146/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 7.9228e-07\n",
            "Epoch 147/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 7.9228e-07\n",
            "Epoch 148/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 6.3383e-07\n",
            "Epoch 149/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 6.3383e-07\n",
            "Epoch 150/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3996 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 6.3383e-07\n",
            "Epoch 151/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 5.0706e-07\n",
            "Epoch 152/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 5.0706e-07\n",
            "Epoch 153/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 5.0706e-07\n",
            "Epoch 154/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 4.0565e-07\n",
            "Epoch 155/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 4.0565e-07\n",
            "Epoch 156/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 4.0565e-07\n",
            "Epoch 157/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 3.2452e-07\n",
            "Epoch 158/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 3.2452e-07\n",
            "Epoch 159/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0816 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3996 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 3.2452e-07\n",
            "Epoch 160/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 2.5961e-07\n",
            "Epoch 161/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 2.5961e-07\n",
            "Epoch 162/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 2.5961e-07\n",
            "Epoch 163/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 2.0769e-07\n",
            "Epoch 164/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 2.0769e-07\n",
            "Epoch 165/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 2.0769e-07\n",
            "Epoch 166/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.6615e-07\n",
            "Epoch 167/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.6615e-07\n",
            "Epoch 168/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.6615e-07\n",
            "Epoch 169/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.3292e-07\n",
            "Epoch 170/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.3292e-07\n",
            "Epoch 171/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.3292e-07\n",
            "Epoch 172/200\n",
            "2914/2914 [==============================] - 43s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.0634e-07\n",
            "Epoch 173/200\n",
            "2914/2914 [==============================] - 42s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.0634e-07\n",
            "Epoch 174/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.0634e-07\n",
            "Epoch 175/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 8.5071e-08\n",
            "Epoch 176/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 8.5071e-08\n",
            "Epoch 177/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 8.5071e-08\n",
            "Epoch 178/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 6.8056e-08\n",
            "Epoch 179/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 6.8056e-08\n",
            "Epoch 180/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 6.8056e-08\n",
            "Epoch 181/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 5.4445e-08\n",
            "Epoch 182/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 5.4445e-08\n",
            "Epoch 183/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 5.4445e-08\n",
            "Epoch 184/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 4.3556e-08\n",
            "Epoch 185/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 4.3556e-08\n",
            "Epoch 186/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 4.3556e-08\n",
            "Epoch 187/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 3.4845e-08\n",
            "Epoch 188/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 3.4845e-08\n",
            "Epoch 189/200\n",
            "2914/2914 [==============================] - 41s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 3.4845e-08\n",
            "Epoch 190/200\n",
            "2914/2914 [==============================] - 42s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 2.7876e-08\n",
            "Epoch 191/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 2.7876e-08\n",
            "Epoch 192/200\n",
            "2914/2914 [==============================] - 40s 14ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 2.7876e-08\n",
            "Epoch 193/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 2.2301e-08\n",
            "Epoch 194/200\n",
            "2914/2914 [==============================] - 40s 13ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 2.2301e-08\n",
            "Epoch 195/200\n",
            "2914/2914 [==============================] - 38s 13ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 2.2301e-08\n",
            "Epoch 196/200\n",
            "2914/2914 [==============================] - 38s 13ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.7841e-08\n",
            "Epoch 197/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.7841e-08\n",
            "Epoch 198/200\n",
            "2914/2914 [==============================] - 39s 13ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.7841e-08\n",
            "Epoch 199/200\n",
            "2914/2914 [==============================] - 38s 13ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.4272e-08\n",
            "Epoch 200/200\n",
            "2914/2914 [==============================] - 38s 13ms/step - loss: 1.0815 - mse: 0.1107 - mae: 0.0460 - val_loss: 2.3997 - val_mse: 2.3450 - val_mae: 1.1796 - lr: 1.4272e-08\n"
          ]
        }
      ],
      "source": [
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "def model_creation():\n",
        "\n",
        "  inputs = Input(shape=[window_size,8])\n",
        "  close_val = tf.expand_dims(inputs[:,0,7],-1)\n",
        "  inputs2 = inputs[:,:,:-1]\n",
        "  x = LSTM(128,input_shape=[window_size,7],kernel_initializer = tf.keras.initializers.RandomNormal(mean=0,stddev=1e-6))(inputs2)\n",
        "  x = Dense(128,activation='relu',kernel_initializer = tf.keras.initializers.RandomNormal(mean=0,stddev=1e-6))(x)\n",
        "  x = Dense(1,activation='tanh',kernel_initializer = tf.keras.initializers.RandomNormal(mean=0,stddev=1e-6))(x)\n",
        "\n",
        "  y = close_val\n",
        "  x  = y * (1 + x)\n",
        "\n",
        "  model = Model(inputs=inputs,outputs=x)\n",
        "  return model\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "model = model_creation()\n",
        "plateau_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"loss\",factor=0.8,patience=3,min_lr=1e-10)\n",
        "model.compile(loss=\"mape\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=['mse','mae'])\n",
        "history = model.fit(training_data,epochs=200,validation_data=validation_data,callbacks=[plateau_lr])\n",
        "\n",
        "model.save(os.path.join(os.getcwd(),'stock_model.keras'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training and validation losses are quite close to the adhoc's loss although the model was iterated along $200$ epochs and the model is sufficiently complex. This might indicate that the information passed to the model is not sufficient to explain the variance in the response. The next step will be to add additional information to the input."
      ],
      "metadata": {
        "id": "FmLp7yWIZtLJ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}